{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://github.com/georgeyiasemis/Recurrent-Neural-Networks-from-scratch-using-PyTorch/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.xh = nn.Linear(input_size, hidden_size * 4, bias=bias)\n",
    "        self.hh = nn.Linear(hidden_size, hidden_size * 4, bias=bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "\n",
    "        # Inputs:\n",
    "        #       input: of shape (batch_size, input_size)\n",
    "        #       hx: of shape (batch_size, hidden_size)\n",
    "        # Outputs:\n",
    "        #       hy: of shape (batch_size, hidden_size)\n",
    "        #       cy: of shape (batch_size, hidden_size)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
    "            hx = (hx, hx)\n",
    "\n",
    "        hx, cx = hx\n",
    "\n",
    "        gates = self.xh(input) + self.hh(hx)\n",
    "\n",
    "        # Get gates (i_t, f_t, g_t, o_t)\n",
    "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, 1)\n",
    "\n",
    "        i_t = torch.sigmoid(input_gate)\n",
    "        f_t = torch.sigmoid(forget_gate)\n",
    "        g_t = torch.tanh(cell_gate)\n",
    "        o_t = torch.sigmoid(output_gate)\n",
    "\n",
    "        cy = cx * f_t + i_t * g_t\n",
    "\n",
    "        hy = o_t * torch.tanh(cy)\n",
    "\n",
    "\n",
    "        return (hy, cy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "\n",
    "        self.rnn_cell_list.append(LSTMCell(self.input_size,\n",
    "                                            self.hidden_size,\n",
    "                                            self.bias))\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.rnn_cell_list.append(LSTMCell(self.hidden_size,\n",
    "                                                self.hidden_size,\n",
    "                                                self.bias))\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "\n",
    "        # Input of shape (batch_size, seqence length , input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        if hx is None:\n",
    "            if torch.cuda.is_available():\n",
    "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size).cuda())\n",
    "            else:\n",
    "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size))\n",
    "        else:\n",
    "             h0 = hx\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        hidden = list()\n",
    "        for layer in range(self.num_layers):\n",
    "            hidden.append((h0[layer, :, :], h0[layer, :, :]))\n",
    "\n",
    "        for t in range(input.size(1)):\n",
    "\n",
    "            for layer in range(self.num_layers):\n",
    "\n",
    "                if layer == 0:\n",
    "                    hidden_l = self.rnn_cell_list[layer](input[:, t, :], (hidden[layer][0], hidden[layer][1]))\n",
    "                else:\n",
    "                    hidden_l = self.rnn_cell_list[layer](hidden[layer - 1][0], (hidden[layer][0], hidden[layer][1]))\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "            outs.append(hidden_l[0])\n",
    "\n",
    "        out = outs[-1].squeeze()\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=\"tanh\"):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.nonlinearity = nonlinearity\n",
    "        if self.nonlinearity not in [\"tanh\", \"relu\"]:\n",
    "            raise ValueError(\"Invalid nonlinearity selected for RNN.\")\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "\n",
    "        # Inputs:\n",
    "        #       input: of shape (batch_size, input_size)\n",
    "        #       hx: of shape (batch_size, hidden_size)\n",
    "        # Output:\n",
    "        #       hy: of shape (batch_size, hidden_size)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
    "\n",
    "        hy = (self.x2h(input) + self.h2h(hx))\n",
    "\n",
    "        if self.nonlinearity == \"tanh\":\n",
    "            hy = torch.tanh(hy)\n",
    "        else:\n",
    "            hy = torch.relu(hy)\n",
    "\n",
    "        return hy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size, activation='tanh'):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            self.rnn_cell_list.append(RNNCell(self.input_size,\n",
    "                                                   self.hidden_size,\n",
    "                                                   self.bias,\n",
    "                                                   \"tanh\"))\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(RNNCell(self.hidden_size,\n",
    "                                                       self.hidden_size,\n",
    "                                                       self.bias,\n",
    "                                                       \"tanh\"))\n",
    "\n",
    "        elif activation == 'relu':\n",
    "            self.rnn_cell_list.append(RNNCell(self.input_size,\n",
    "                                                   self.hidden_size,\n",
    "                                                   self.bias,\n",
    "                                                   \"relu\"))\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(RNNCell(self.hidden_size,\n",
    "                                                   self.hidden_size,\n",
    "                                                   self.bias,\n",
    "                                                   \"relu\"))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation.\")\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "\n",
    "        # Input of shape (batch_size, seqence length, input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        if hx is None:\n",
    "            if torch.cuda.is_available():\n",
    "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size).cuda())\n",
    "            else:\n",
    "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size))\n",
    "\n",
    "        else:\n",
    "             h0 = hx\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        hidden = list()\n",
    "        for layer in range(self.num_layers):\n",
    "            hidden.append(h0[layer, :, :])\n",
    "\n",
    "        for t in range(input.size(1)):\n",
    "\n",
    "            for layer in range(self.num_layers):\n",
    "\n",
    "                if layer == 0:\n",
    "                    hidden_l = self.rnn_cell_list[layer](input[:, t, :], hidden[layer])\n",
    "                else:\n",
    "                    hidden_l = self.rnn_cell_list[layer](hidden[layer - 1],hidden[layer])\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "            outs.append(hidden_l)\n",
    "\n",
    "        # Take only last time step. Modify for seq to seq\n",
    "        out = outs[-1].squeeze()\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "\n",
    "        # Inputs:\n",
    "        #       input: of shape (batch_size, input_size)\n",
    "        #       hx: of shape (batch_size, hidden_size)\n",
    "        # Output:\n",
    "        #       hy: of shape (batch_size, hidden_size)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
    "\n",
    "        x_t = self.x2h(input)\n",
    "        h_t = self.h2h(hx)\n",
    "\n",
    "\n",
    "        x_reset, x_upd, x_new = x_t.chunk(3, 1)\n",
    "        h_reset, h_upd, h_new = h_t.chunk(3, 1)\n",
    "\n",
    "        reset_gate = torch.sigmoid(x_reset + h_reset)\n",
    "        update_gate = torch.sigmoid(x_upd + h_upd)\n",
    "        new_gate = torch.tanh(x_new + (reset_gate * h_new))\n",
    "\n",
    "        hy = update_gate * hx + (1 - update_gate) * new_gate\n",
    "\n",
    "        return hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "\n",
    "        self.rnn_cell_list.append(GRUCell(self.input_size,\n",
    "                                          self.hidden_size,\n",
    "                                          self.bias))\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.rnn_cell_list.append(GRUCell(self.hidden_size,\n",
    "                                              self.hidden_size,\n",
    "                                              self.bias))\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "\n",
    "        # Input of shape (batch_size, seqence length, input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        if hx is None:\n",
    "            if torch.cuda.is_available():\n",
    "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size).cuda())\n",
    "            else:\n",
    "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size))\n",
    "\n",
    "        else:\n",
    "             h0 = hx\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        hidden = list()\n",
    "        for layer in range(self.num_layers):\n",
    "            hidden.append(h0[layer, :, :])\n",
    "\n",
    "        for t in range(input.size(1)):\n",
    "\n",
    "            for layer in range(self.num_layers):\n",
    "\n",
    "                if layer == 0:\n",
    "                    hidden_l = self.rnn_cell_list[layer](input[:, t, :], hidden[layer])\n",
    "                else:\n",
    "                    hidden_l = self.rnn_cell_list[layer](hidden[layer - 1],hidden[layer])\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "            outs.append(hidden_l)\n",
    "\n",
    "        # Take only last time step. Modify for seq to seq\n",
    "        out = outs[-1].squeeze()\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirRecurrentModel(nn.Module):\n",
    "    def __init__(self, mode, input_size, hidden_size, num_layers, bias, output_size):\n",
    "        super(BidirRecurrentModel, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "\n",
    "        if mode == 'LSTM':\n",
    "\n",
    "            self.rnn_cell_list.append(LSTMCell(self.input_size,\n",
    "                                              self.hidden_size,\n",
    "                                              self.bias))\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(LSTMCell(self.hidden_size,\n",
    "                                                    self.hidden_size,\n",
    "                                                    self.bias))\n",
    "\n",
    "        elif mode == 'GRU':\n",
    "            self.rnn_cell_list.append(GRUCell(self.input_size,\n",
    "                                              self.hidden_size,\n",
    "                                              self.bias))\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(GRUCell(self.hidden_size,\n",
    "                                                  self.hidden_size,\n",
    "                                                  self.bias))\n",
    "\n",
    "        elif mode == 'RNN_TANH':\n",
    "            self.rnn_cell_list.append(RNNCell(self.input_size,\n",
    "                                                   self.hidden_size,\n",
    "                                                   self.bias,\n",
    "                                                   \"tanh\"))\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(RNNCell(self.hidden_size,\n",
    "                                                       self.hidden_size,\n",
    "                                                       self.bias,\n",
    "                                                       \"tanh\"))\n",
    "\n",
    "        elif mode == 'RNN_RELU':\n",
    "            self.rnn_cell_list.append(RNNCell(self.input_size,\n",
    "                                                   self.hidden_size,\n",
    "                                                   self.bias,\n",
    "                                                   \"relu\"))\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(RNNCell(self.hidden_size,\n",
    "                                                   self.hidden_size,\n",
    "                                                   self.bias,\n",
    "                                                   \"relu\"))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN mode selected.\")\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "\n",
    "        # Input of shape (batch_size, sequence length, input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            hT = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size).cuda())\n",
    "        else:\n",
    "            hT = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size))\n",
    "\n",
    "        outs = []\n",
    "        outs_rev = []\n",
    "\n",
    "        hidden_forward = list()\n",
    "        for layer in range(self.num_layers):\n",
    "            if self.mode == 'LSTM':\n",
    "                hidden_forward.append((h0[layer, :, :], h0[layer, :, :]))\n",
    "            else:\n",
    "                hidden_forward.append(h0[layer, :, :])\n",
    "\n",
    "        hidden_backward = list()\n",
    "        for layer in range(self.num_layers):\n",
    "            if self.mode == 'LSTM':\n",
    "                hidden_backward.append((hT[layer, :, :], hT[layer, :, :]))\n",
    "            else:\n",
    "                hidden_backward.append(hT[layer, :, :])\n",
    "\n",
    "        for t in range(input.shape[1]):\n",
    "            for layer in range(self.num_layers):\n",
    "\n",
    "                if self.mode == 'LSTM':\n",
    "                    # If LSTM\n",
    "                    if layer == 0:\n",
    "                        # Forward net\n",
    "                        h_forward_l = self.rnn_cell_list[layer](\n",
    "                            input[:, t, :],\n",
    "                            (hidden_forward[layer][0], hidden_forward[layer][1])\n",
    "                            )\n",
    "                        # Backward net\n",
    "                        h_back_l = self.rnn_cell_list[layer](\n",
    "                            input[:, -(t + 1), :],\n",
    "                            (hidden_backward[layer][0], hidden_backward[layer][1])\n",
    "                            )\n",
    "                    else:\n",
    "                        # Forward net\n",
    "                        h_forward_l = self.rnn_cell_list[layer](\n",
    "                            hidden_forward[layer - 1][0],\n",
    "                            (hidden_forward[layer][0], hidden_forward[layer][1])\n",
    "                            )\n",
    "                        # Backward net\n",
    "                        h_back_l = self.rnn_cell_list[layer](\n",
    "                            hidden_backward[layer - 1][0],\n",
    "                            (hidden_backward[layer][0], hidden_backward[layer][1])\n",
    "                            )\n",
    "\n",
    "                else:\n",
    "                    # If RNN{_TANH/_RELU} / GRU\n",
    "                    if layer == 0:\n",
    "                        # Forward net\n",
    "                        h_forward_l = self.rnn_cell_list[layer](input[:, t, :], hidden_forward[layer])\n",
    "                        # Backward net\n",
    "                        h_back_l = self.rnn_cell_list[layer](input[:, -(t + 1), :], hidden_backward[layer])\n",
    "                    else:\n",
    "                        # Forward net\n",
    "                        h_forward_l = self.rnn_cell_list[layer](hidden_forward[layer - 1], hidden_forward[layer])\n",
    "                        # Backward net\n",
    "                        h_back_l = self.rnn_cell_list[layer](hidden_backward[layer - 1], hidden_backward[layer])\n",
    "\n",
    "\n",
    "                hidden_forward[layer] = h_forward_l\n",
    "                hidden_backward[layer] = h_back_l\n",
    "\n",
    "            if self.mode == 'LSTM':\n",
    "\n",
    "                outs.append(h_forward_l[0])\n",
    "                outs_rev.append(h_back_l[0])\n",
    "\n",
    "            else:\n",
    "                outs.append(h_forward_l)\n",
    "                outs_rev.append(h_back_l)\n",
    "\n",
    "        # Take only last time step. Modify for seq to seq\n",
    "        out = outs[-1].squeeze()\n",
    "        out_rev = outs_rev[0].squeeze()\n",
    "        out = torch.cat((out, out_rev), 1)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
